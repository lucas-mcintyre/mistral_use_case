# Mistral AI Product Classification Use Case

This project implements and evaluates multiple baseline approaches for **product category classification** using French Amazon catalog data and Mistral AI models. 

## üìÅ Project Structure

```
lm_mistral_use_case/
‚îú‚îÄ‚îÄ data_raw/                    # Raw data files
‚îÇ   ‚îî‚îÄ‚îÄ fr_amazon_catalog.csv   # French Amazon product catalog
‚îú‚îÄ‚îÄ data_processed/              # Processed data files
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl             # Training data
‚îÇ   ‚îú‚îÄ‚îÄ val.jsonl               # Validation data
‚îÇ   ‚îú‚îÄ‚îÄ test.jsonl              # Test data
‚îÇ   ‚îî‚îÄ‚îÄ mappings/               # Label mappings
‚îÇ       ‚îú‚îÄ‚îÄ leaf2id.json        # Leaf category to ID mapping
‚îÇ       ‚îî‚îÄ‚îÄ id2path.json        # ID mapping to category path
‚îú‚îÄ‚îÄ datascripts/                # Data preprocessing scripts
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py         # Main data preparation
‚îÇ   ‚îî‚îÄ‚îÄ text_cleaning.py        # Text cleaning utilities
‚îú‚îÄ‚îÄ src/baselines/              # Classification baselines
‚îÇ   ‚îú‚îÄ‚îÄ naive_yes_no_baseline.py      # Zero-shot yes/no classification
‚îÇ   ‚îú‚îÄ‚îÄ classical_baseline.py          # TF-IDF + Logistic Regression
‚îÇ   ‚îú‚îÄ‚îÄ improved_RAG_baseline.py      # Dual retrieval + single prompt
‚îÇ   ‚îú‚îÄ‚îÄ fine_tune_mistral.py          # Mistral fine-tuning
‚îÇ   ‚îú‚îÄ‚îÄ check_cache.py                # Cache verification utility
‚îÇ   ‚îî‚îÄ‚îÄ README_parameter_search.md    # Parameter search guide
‚îú‚îÄ‚îÄ src/baselines/grid_search_for_RAG/  # Parameter search tools
‚îÇ   ‚îú‚îÄ‚îÄ quick_param_test_extended.py   # Extended quick parameter exploration for RAG
‚îÇ   ‚îî‚îÄ‚îÄ comprehensive_grid_search.py   # Full grid search with cost analysis
‚îú‚îÄ‚îÄ src/api/                   # FastAPI server
‚îÇ   ‚îú‚îÄ‚îÄ main.py                # API server implementation
‚îÇ   ‚îî‚îÄ‚îÄ client.py              # API client for testing
‚îú‚îÄ‚îÄ eval_lib/                   # Evaluation utilities
‚îÇ   ‚îî‚îÄ‚îÄ core.py                # Evaluation helpers and CLI
‚îú‚îÄ‚îÄ models/                     # Model outputs and caches
‚îÇ   ‚îú‚îÄ‚îÄ naive_yes_no/          # Zero-shot baseline results
‚îÇ   ‚îú‚îÄ‚îÄ classical/             # Classical baseline results
‚îÇ   ‚îú‚îÄ‚îÄ improved_RAG/          # RAG baseline results and caches
‚îÇ   ‚îî‚îÄ‚îÄ fine_tune/             # Fine-tuning results
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ 01_eda.ipynb          # Exploratory data analysis
‚îî‚îÄ‚îÄ requirements.txt            # Python dependencies
```

## üöÄ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
export MISTRAL_API_KEY="your-mistral-api-key-here"
```

### 2. Data Preparation

> **Note:**  
> You must first download the dataset manually from Kaggle:  
> [https://www.kaggle.com/datasets/promptcloud/amazon-france-product-dataset](https://www.kaggle.com/datasets/promptcloud/amazon-france-product-dataset)  
>  
> Place the downloaded CSV file (e.g., `fr_amazon_catalog.csv`) in the `data_raw/` directory before running the data preparation scripts below.

```bash
# Prepare main dataset (filters parent paths and categories with <8 products)
python datascripts/prepare_data.py \
    --input data_raw/fr_amazon_catalog.csv \
    --output_dir data_processed_filtered \
    --val_frac 0.10 \
    --test_frac 0.10 \
    --seed 42
```

### 3. Run Baselines

```bash
# Zero-shot yes/no baseline
python src/baselines/naive_yes_no_baseline.py \
    --test data_processed_filtered/test.jsonl \
    --mapping data_processed_filtered/mappings/leaf2id.json \
    --model mistral-small-latest \
    --output_dir models/naive_yes_no \

# Classical baseline (TF-IDF + Logistic Regression)
python src/baselines/classical_baseline.py \
    --train data_processed_filtered/train.jsonl \
    --val data_processed_filtered/val.jsonl \
    --model_dir models/classical \
    --mode train

# Evaluate classical baseline
python src/baselines/classical_baseline.py \
    --test data_processed/test.jsonl \
    --model_dir models/classical \
    --mode eval

# Quick test with limited data
python src/baselines/classical_baseline.py \
    --train data_processed_filtered/train.jsonl \
    --val data_processed_filtered/val.jsonl \
    --model_dir models/classical_quick \
    --mode train \
    --max_samples 1000 \
    --max_classes 50

# Dual retrieval RAG baseline
python src/baselines/improved_RAG_baseline.py \
    --test data_processed_filtered/test.jsonl \
    --mapping data_processed_filtered/mappings/leaf2id.json \
    --model mistral-small-latest \
    --output_dir models/improved_RAG \
    --k_tfidf 25 \
    --k_embed 25 \
    --limit 100

# Parameter search for optimal k_tfidf and k_embed values
# First, check existing caches
python src/baselines/check_cache.py --cache_dir models/improved_RAG_filtered_dataset

# Quick parameter test (5 combinations)
python src/baselines/grid_search_for_RAG/quick_param_test_extended.py \
    --test data_processed_filtered/test.jsonl \
    --mapping data_processed_filtered/mappings/leaf2id.json \
    --shared_cache models/improved_RAG \
    --limit 50

# Comprehensive grid search across multiple models
python src/baselines/grid_search_for_RAG/comprehensive_grid_search.py \
    --test data_processed_filtered/test.jsonl \
    --mapping data_processed_filtered/mappings/leaf2id.json \
    --shared_cache models/improved_RAG \
    --k_tfidf_range 15 20 25 30 \
    --k_embed_range 15 20 25 30 \
    --models mistral-small-latest mistral-medium-latest mistral-large-latest \
    --limit 100

# Fine-tune Mistral model
python src/baselines/fine_tune_mistral.py \
    --train data_processed_filtered/train.jsonl \
    --val data_processed_filtered/val.jsonl \
    --test data_processed_filtered/test.jsonl \
    --output_dir models/fine_tune \
    --base_model ministral-3b-latest
```

### 4. Evaluate Models

```bash
# Evaluate improved RAG baseline
python -m eval_lib.core improved_rag \
    --test data_processed/test.jsonl \
    --mapping data_processed/mappings/leaf2id.json \
    --output_dir models/improved_RAG \
    --model mistral-small-latest \
    --k_tfidf 15 \
    --k_embed 20 \
    --limit 100 \
    --report

# Evaluate fine-tuned model
python -m eval_lib.core mistral \
    --model ft:classifier:mistral-small-latest:your-model-id \
    --test data_processed/test.jsonl \
    --cache models/fine_tune/ft_cls_cache.joblib \
    --limit 100 \
    --report \
    > src/evaluation_report.txt
```

### 5. Start API Server

```bash
# Start the FastAPI server
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

# Test the API
python src/api/client.py
```

## üåê API Server

### FastAPI Server (`src/api/main.py`)
Provides a REST API for serving the fine-tuned model:

#### **Endpoints:**
- `GET /` - API information
- `GET /health` - Health check
- `GET /model-info` - Model information
- `POST /classify` - Single product classification
- `POST /classify/batch` - Batch classification (with optional limit)
- `POST /classify/limited` - Limited classification (process only N texts)
- `GET /examples` - Example product descriptions
- `GET /cache/stats` - Cache statistics
- `DELETE /cache` - Clear cache

#### **Usage Examples:**

```bash
# Start server
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

# Single classification
curl -X POST "http://localhost:8000/classify" \
     -H "Content-Type: application/json" \
     -d '{"text": "Samsung Galaxy S21 Smartphone", "use_cache": true}'

# Batch classification with limit
curl -X POST "http://localhost:8000/classify/batch" \
     -H "Content-Type: application/json" \
     -d '{"texts": ["Product 1", "Product 2", "Product 3"], "use_cache": true, "limit": 2}'

# Limited classification
curl -X POST "http://localhost:8000/classify/limited" \
     -H "Content-Type: application/json" \
     -d '{"texts": ["Product 1", "Product 2", "Product 3"], "limit": 2, "use_cache": true}'
```

#### **Python Client:**
```python
import requests

# Single classification
response = requests.post("http://localhost:8000/classify", json={
    "text": "Samsung Galaxy S21 Smartphone",
    "use_cache": True
})
result = response.json()
print(f"Predicted: {result['predicted_label']}")

# Batch classification with limit
response = requests.post("http://localhost:8000/classify/batch", json={
    "texts": ["Product 1", "Product 2", "Product 3"],
    "use_cache": True,
    "limit": 2  # Only process first 2 texts
})
results = response.json()
print(f"Processed {len(results['predictions'])} texts")
```

#### **API Documentation:**
Once the server is running, visit:
- **Interactive Docs**: http://localhost:8000/docs
- **Alternative Docs**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health
```

## üìä Baseline Approaches

### 1. Naive Yes/No Baseline (`naive_yes_no_baseline.py`)
- **Strategy**: TF-IDF retrieval + yes/no questions to Mistral
- **Process**: 
  1. Retrieve top-K candidate categories using TF-IDF
  2. Ask Mistral "Does this product belong to category X?" for each candidate
  3. Select first category that gets "Oui" response
- **Pros**: Simple, interpretable, no training required
- **Cons**: Expensive (k x n API calls), sequential processing

### 2. Classical Baseline (`classical_baseline.py`)
- **Strategy**: TF-IDF vectorization + Logistic Regression
- **Process**:
  1. Convert product descriptions to TF-IDF vectors
  2. Train Logistic Regression classifier (one-vs-rest for multi-class)
  3. Predict on test set
- **Features**: 
  - `--max_samples`: Limit training to N samples (for faster testing)
  - `--max_classes`: Limit to N most frequent classes (for faster testing)
- **Pros**: Fast, cheap, well-understood, good baseline performance
- **Cons**: Limited to bag-of-words features, no semantic understanding

### 3. Improved RAG Baseline (`improved_RAG_baseline.py`)
- **Strategy**: Dual retrieval + single prompt classification
- **Process**:
  1. Retrieve candidates using TF-IDF (k_tfidf) + FAISS embeddings (k_embed)
  2. Present all candidates to Mistral in single prompt
  3. Model chooses best category by number
- **Pros**: Combines multiple retrieval methods, single API call per example
- **Cons**: Depends on retrieval quality

### 4. Fine-tuned Mistral (`fine_tune_mistral.py`)
- **Strategy**: Fine-tune Mistral model on product classification
- **Process**:
  1. Prepare training data in chat completion format
  2. Upload to Mistral and create fine-tuning job
  3. Wait for training completion
  4. Use fine-tuned model for classification
- **Pros**: Best potential performance, learns task-specific patterns
- **Cons**: Expensive, requires training time, model-specific, requires API credits

## üîß Data Processing

### Main Data Preparation (`prepare_data.py`)
- Loads French Amazon catalog CSV
- Handles encoding (UTF-16) and delimiter (tab) issues
- Extracts product descriptions and category paths
- **Filters parent paths** when child paths exist (keeps only leaf nodes)
- **Filters categories** with fewer than 8 products in training data
- Splits data into train/val/test sets (80/10/10 split)
- No separation between seen/unseen categories - all categories can appear in any split

## üìà Evaluation

### Evaluation Library (`eval_lib/core.py`)
Provides multiple evaluation modes:

```bash
# Evaluate improved RAG baseline
python -m eval_lib.core improved_rag \
    --test data_processed/test.jsonl \
    --mapping data_processed/mappings/leaf2id.json \
    --output_dir models/improved_RAG \
    --model mistral-small-latest \
    --k_tfidf 15 \
    --k_embed 20 \
    --limit 100 \
    --report

# Evaluate fine-tuned model
python -m eval_lib.core mistral \
    --model ft:classifier:mistral-small-latest:your-model-id \
    --test data_processed/test.jsonl \
    --cache models/fine_tune/ft_cls_cache.joblib \
    --limit 1000 \
    --report \
    > src/evaluation_report.txt

# Evaluate file predictions
python -m eval_lib.core files --ref labels.txt --pred predictions.txt --report

# Evaluate with ID mappings
python -m eval_lib.core mappings --id2path id2path.json --preds predictions.json --report
```

#### **Python Client:**
```python
import requests

# Single classification
response = requests.post("http://localhost:8000/classify", json={
    "text": "Samsung Galaxy S21 Smartphone",
    "use_cache": True
})
result = response.json()
print(f"Predicted: {result['predicted_label']}")

# Batch classification with limit
response = requests.post("http://localhost:8000/classify/batch", json={
    "texts": ["Product 1", "Product 2", "Product 3"],
    "use_cache": True,
    "limit": 2  # Only process first 2 texts
})
results = response.json()
print(f"Processed {len(results['predictions'])} texts")
```

### Metrics
- **Accuracy**: Overall correct predictions
- **F1 Micro**: Micro-averaged F1 score
- **F1 Macro**: Macro-averaged F1 score
- **Precision/Recall**: Micro-averaged precision and recall
- **Confusion Matrix**: For top 15 most frequent predictions

### Progress Tracking
All scripts include comprehensive progress tracking:
- Example-by-example progress
- Cache hit vs API call indicators
- Running accuracy statistics
- Detailed error analysis

## üíæ Caching

All baselines implement intelligent caching:
- **API Responses**: Cached to avoid duplicate API calls
- **Embeddings**: FAISS index and embeddings cached on disk
- **Model Predictions**: Cached for fine-tuned model evaluation
- **Shared Caches**: Reuse caches across parameter searches (see `src/baselines/README_parameter_search.md`)
- **Cache Management**: Clear cache with `--clear-cache` flag

### Parameter Search Pipeline

The project includes a comprehensive parameter search pipeline for optimizing the improved RAG model:

#### **1. Cache Verification**
```bash
# Check existing caches before parameter search
python src/baselines/check_cache.py --cache_dir models/improved_RAG
```

#### **2. Quick Parameter Test**
```bash
# Test 5 key combinations to understand parameter space
python src/baselines/quick_param_test.py \
    --test data_processed/test.jsonl \
    --mapping data_processed/mappings/leaf2id.json \
    --shared_cache models/improved_RAG \
    --limit 50
```

#### **3. Comprehensive Grid Search**
```bash
# Full grid search across multiple models with cost analysis
python src/baselines/comprehensive_grid_search.py \
    --test data_processed/test.jsonl \
    --mapping data_processed/mappings/leaf2id.json \
    --shared_cache models/improved_RAG \
    --k_tfidf_range 15 20 25 30 \
    --k_embed_range 15 20 25 30 \
    --models mistral-small-latest mistral-medium-latest mistral-large-latest \
    --limit 100
```

#### **Key Features:**
- **Shared Caches**: Reuses FAISS index, embeddings, and LLM responses
- **Cost Analysis**: Estimates API costs for each parameter combination
- **Multi-Model Support**: Tests across small, medium, and large Mistral models
- **Visualization**: Generates plots showing F1 scores vs cost
- **Efficient**: Avoids redundant computations through smart caching

#### **Output:**
- Performance metrics for each combination
- Cost estimates and analysis
- Visualization plots (F1 scores, cost heatmaps)
- Recommendations for optimal parameters

For detailed workflow and troubleshooting, see `src/baselines/README_parameter_search.md`

## üêõ Troubleshooting

### Common Issues

1. **Encoding Errors**: Ensure CSV is read with `encoding='utf-16'`
2. **API Key Issues**: Set `MISTRAL_API_KEY` environment variable
3. **Model Name Errors**: Use correct model names (e.g., `mistral-small-latest` not `mistral-small`)
4. **Memory Issues**: Use `--limit` flag to process fewer examples
5. **Network Errors**: Retry failed API calls, check internet connection

### Debug Mode
All scripts include verbose progress tracking. Look for:
- `[SETUP]` - Initialization messages
- `[EVAL]` - Evaluation progress
- `[CACHE]` - Cache hit indicators
- `[API]` - API call indicators
- `[ERROR]` - Error messages

## üìù Example Output

### Data Preparation
```
Reading raw ‚Ä¶
Cleaning text ‚Ä¶
Original training paths: 2,333
After filtering parent paths: 2,020

Filtering categories by minimum product count...
Categories with ‚â•8 products: 410
Categories that are leaf nodes AND have ‚â•8 products: 369
After filtering data:
  Train rows: 15,941 (was 22,957)
  Val rows: 1,976 (was 2,870)
  Test rows: 1,950 (was 2,870)

Split summary
Total unique paths: 2,560
Number of products: 28,697
Final unique paths (leaf nodes only): 369
```

### Classical Baseline Training
```
[MODEL] Training Logistic Regression classifier...
[MODEL] Number of classes: 369
[MODEL] Fitting model (this may take a while)...
[MODEL] Progress will be shown below:
[LibLinear] 1 2 3 4 5...
[MODEL] Training completed in 45.23 seconds!
[MODEL] Average time per class: 0.123 seconds
```

### Improved RAG Evaluation
```
[SETUP] Output directory: models/improved_RAG
[SETUP] Initialized caches: LLM=llm_cache.joblib, Embed=embed_cache.joblib
[BUILD] Building retrieval indices...
[EVAL] Example 1/100:
  Text: Samsung Galaxy S21 Smartphone with 128GB Storage...
  True label: Smartphones
  [RETRIEVAL] Found 18 unique candidates
  [LLM] Model chose candidate 3: Smartphones
  [RESULT] ‚úÖ CORRECT (true: Smartphones)
  [STATS] Running accuracy: 1/1 = 1.000

[METRICS] Final Results:
{
  "retrieval_recall": 0.95,
  "f1_micro": 0.87,
  "f1_macro": 0.82
}
```

### Parameter Search Results
```
üîç CACHE VERIFICATION:
‚úÖ faiss_index.bin      FAISS vector index   6.3 MB
‚úÖ embed_cache.joblib   Embedding cache      14.3 MB
‚úÖ llm_cache.joblib     LLM response cache   0.0 MB

üöÄ QUICK PARAMETER TEST:
Testing 5 combinations...
ü•á Best F1 Micro: k_tfidf=10, k_embed=10
   F1 Micro: 0.7845, F1 Macro: 0.7234
   Retrieval Recall: 0.9600
   Total Candidates: 20

üìä COMPREHENSIVE GRID SEARCH:
Testing 16 combinations across 3 models...
üèÜ BEST COMBINATIONS:
ü•á mistral-large-latest: k_tfidf=25, k_embed=25
   F1 Micro: 0.8234, Cost: $12.45
ü•à mistral-medium-latest: k_tfidf=20, k_embed=20  
   F1 Micro: 0.8123, Cost: $8.23
ü•â mistral-small-latest: k_tfidf=15, k_embed=15
   F1 Micro: 0.7945, Cost: $4.12

üí° RECOMMENDATIONS:
- For best performance: mistral-large-latest with k_tfidf=25, k_embed=25
- For cost efficiency: mistral-small-latest with k_tfidf=15, k_embed=15
- For balanced approach: mistral-medium-latest with k_tfidf=20, k_embed=20
```

## ü§ù Contributing

1. Add new baselines to `src/baselines/`
2. Update evaluation metrics in `eval_lib/core.py`
3. Add data processing scripts to `datascripts/`
4. Document changes in this README

## üìÑ License

This project is for educational and research purposes. Please respect Mistral AI's terms of service and API usage limits.
